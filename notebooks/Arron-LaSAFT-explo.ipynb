{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy\n",
    "import librosa\n",
    "import youtube_dl\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing directory to use the model package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../models/Conditioned-Source-Separation-LaSAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model and load it's pretrained 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasaft.source_separation.conditioned.cunet.models.dcun_tfc_gpocm_lasaft import DCUN_TFC_GPoCM_LaSAFT_Framework\n",
    "\n",
    "args = {}\n",
    "\n",
    "# FFT params\n",
    "args['n_fft'] = 2048\n",
    "args['hop_length'] = 1024\n",
    "args['num_frame'] = 128\n",
    "\n",
    "# SVS Framework\n",
    "args['spec_type'] = 'complex'\n",
    "args['spec_est_mode'] = 'mapping'\n",
    "\n",
    "# Other Hyperparams\n",
    "args['optimizer'] = 'adam'\n",
    "args['lr'] = 0.001\n",
    "args['dev_mode'] = False\n",
    "args['train_loss'] = 'spec_mse'\n",
    "args['val_loss'] = 'raw_l1'\n",
    "\n",
    "# DenseNet Hyperparams\n",
    "\n",
    "args ['n_blocks'] = 7\n",
    "args ['input_channels'] = 4\n",
    "args ['internal_channels'] = 24\n",
    "args ['first_conv_activation'] = 'relu'\n",
    "args ['last_activation'] = 'identity'\n",
    "args ['t_down_layers'] = None\n",
    "args ['f_down_layers'] = None\n",
    "args ['tif_init_mode'] = None\n",
    "\n",
    "# TFC_TDF Block's Hyperparams\n",
    "args['n_internal_layers'] =5\n",
    "args['kernel_size_t'] = 3\n",
    "args['kernel_size_f'] = 3\n",
    "args['tfc_tdf_activation'] = 'relu'\n",
    "args['bn_factor'] = 16\n",
    "args['min_bn_units'] = 16\n",
    "args['tfc_tdf_bias'] = True\n",
    "args['num_tdfs'] = 6\n",
    "args['dk'] = 32\n",
    "\n",
    "args['control_vector_type'] = 'embedding'\n",
    "args['control_input_dim'] = 4\n",
    "args['embedding_dim'] = 32\n",
    "args['condition_to'] = 'decoder'\n",
    "\n",
    "args['control_n_layer'] = 4\n",
    "args['control_type'] = 'dense'\n",
    "args['pocm_type'] = 'matmul'\n",
    "args['pocm_norm'] = 'batch_norm'\n",
    "\n",
    "\n",
    "model = DCUN_TFC_GPoCM_LaSAFT_Framework(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_from_checkpoint('pretrained/gpocm_lasaft.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load track with librosa and display a player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_dir = '../../raw_data/musdb18/test/'\n",
    "track_name = 'Tom McKenzie - Directions.stem.mp4'\n",
    "\n",
    "audio, rate = librosa.load(f'{track_dir}{track_name}', mono=False, sr=44100)\n",
    "\n",
    "audio = audio[:, 40*rate:50*rate]\n",
    "\n",
    "display(Audio(audio, rate=rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated = model.separate_track(audio.T, 'vocals') \n",
    "vocals, sr=librosa.load('temp.wav', mono=False)\n",
    "display(Audio('temp.wav')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def separate_all (audio):\n",
    "    '''\n",
    "    you can add or remove what ever you want in this loop\n",
    "    using the keys, vocals, drums, bass and other\n",
    "    you can also combine them \n",
    "    '''\n",
    "    \n",
    "    print('vocals')\n",
    "    separated = model.separate_track(audio.T, 'vocals') \n",
    "    vocals, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('drums')\n",
    "    separated = model.separate_track(audio.T, 'drums') \n",
    "    drums, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('bass')\n",
    "    separated = model.separate_track(audio.T, 'bass') \n",
    "    bass, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('other')\n",
    "    separated = model.separate_track(audio.T, 'other') \n",
    "    other, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "    \n",
    "    print('vocal-backing')\n",
    "    librosa.output.write_wav('temp.wav', drums+bass+other, sr)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('v+d+b+o')\n",
    "    librosa.output.write_wav('temp.wav', vocals+drums+bass+other, sr)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "separate_all(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with songs outside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "url = \"dQw4w9WgXcQ\" #@param {type:\"string\"}\n",
    "start = 42 #@param {type:\"number\"}\n",
    "stop = 52 #@param {type:\"number\"}\n",
    "embed_url = \"https://www.youtube.com/embed/%s?rel=0&start=%d&end=%d&amp;controls=0&amp;showinfo=0\" % (url, start, stop)\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=' + embed_url + 'frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hook(d):\n",
    "    if d['status'] == 'finished':\n",
    "        print('Done downloading...')\n",
    "\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav',\n",
    "        'preferredquality': '44100',\n",
    "    }],\n",
    "    'outtmpl': '%(title)s.wav',\n",
    "    'progress_hooks': [my_hook],\n",
    "}\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(url, download=False)\n",
    "    status = ydl.download([url])\n",
    "\n",
    "audio, rate = librosa.load(info.get('title', None) + '.wav', sr=44100, mono=False)\n",
    "audio = audio[:, start*rate:stop*rate]\n",
    "print(audio.shape)\n",
    "display(Audio(audio, rate=rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_all (audio):\n",
    "    print('vocals')\n",
    "    separated = model.separate_track(audio.T, 'vocals') \n",
    "    vocals, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('drums')\n",
    "    separated = model.separate_track(audio.T, 'drums') \n",
    "    drums, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav'))\n",
    "\n",
    "    print('bass')\n",
    "    separated = model.separate_track(audio.T, 'bass') \n",
    "    bass, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('other')\n",
    "    separated = model.separate_track(audio.T, 'other') \n",
    "    other, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "    \n",
    "    print('vocal-backing')\n",
    "    librosa.output.write_wav('temp.wav', drums+bass+other, sr)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "\n",
    "    print('v+d+b+o')\n",
    "    librosa.output.write_wav('temp.wav', vocals+drums+bass+other, sr)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "separate_all(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using museval to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "track_dir = '../../raw_data/musdb18/test/'\n",
    "track_name = 'Tom McKenzie - Directions.stem.mp4'\n",
    "\n",
    "audio, rate = librosa.load(f'{track_dir}{track_name}', mono=False)\n",
    "\n",
    "# display(Audio(audio, rate=rate))\n",
    "\n",
    "# predictions = []\n",
    "\n",
    "def separate_all (audio):\n",
    "    print('vocals')\n",
    "    separated = model.separate_track(audio.T, 'vocals') \n",
    "    vocals, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('drums')\n",
    "    separated = model.separate_track(audio.T, 'drums') \n",
    "    drums, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('bass')\n",
    "    separated = model.separate_track(audio.T, 'bass') \n",
    "    bass, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('other')\n",
    "    separated = model.separate_track(audio.T, 'other') \n",
    "    other, sr=librosa.load('temp.wav', mono=False)\n",
    "    display(Audio('temp.wav')) \n",
    "    \n",
    "    print('vocal-backing')\n",
    "    librosa.output.write_wav('temp.wav', drums+bass+other, sr)\n",
    "    display(Audio('temp.wav')) \n",
    "\n",
    "    print('v+d+b+o')\n",
    "    librosa.output.write_wav('temp.wav', vocals+drums+bass+other, sr)\n",
    "    display(Audio('temp.wav')) \n",
    "    \n",
    "#     predictions.append([vocals, drums, bass, other])\n",
    "    \n",
    "separate_all(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(predictions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(predictions[0][2], rate=rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
