{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from model.tasnet import MultiTasNet\n",
    "import librosa\n",
    "import youtube_dl\n",
    "import os\n",
    "import IPython.display\n",
    "# from evaluate import separate_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\"best_model.pt\", map_location=torch.device(\"cpu\"))  # load checkpoint\n",
    "network = MultiTasNet(state[\"args\"]).to(device)  # initialize the model\n",
    "network.load_state_dict(state['state_dict'])  # load the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sample(track):\n",
    "    rate = track.rate\n",
    "    \n",
    "    audio = track.audio.astype('float32').transpose(1, 0)\n",
    "    mix = [librosa.core.resample(audio, 44100, s, res_type='kaiser_best', fix=False) for s in[8000, 16000, 32000]]\n",
    "    mix = [librosa.util.fix_length(m, (mix[0].shape[-1]+1)*(2**i)) for i, m in enumerate(mix)]\n",
    "    mix = [torch.from_numpy(s).float().to(device).unsqueeze_(1) for s in mix]\n",
    "    mix = [s / s.std(dim=-1, keepdim=True) for s in mix]\n",
    "\n",
    "    mix_left = [s[0:1, :, :] for s in mix]\n",
    "    mix_right = [s[1:2, :, :] for s in mix]\n",
    "    del mix\n",
    "    mix = mix_left\n",
    "    \n",
    "    def resample(audio, target_rate):\n",
    "        return librosa.core.resample(audio, rate, target_rate, res_type='kaiser_best', fix=False)\n",
    "    \n",
    "#     audio = audio.astype('float32')  # match the type with the type of the weights in the network\n",
    "#     mix = [resample(audio, s) for s in[8000, 16000, 32000]]  # resample to different sampling rates for the three stages\n",
    "#     mix = [librosa.util.fix_length(m, (mix[0].shape[-1]+1)*(2**i)) for i,m in enumerate(mix)]  # allign all three sample so that their lenghts are divisible\n",
    "#     mix = [torch.from_numpy(s).float().to(device).unsqueeze_(1) for s in mix]  # cast to tensor with shape: [1, 1, T']\n",
    "#     mix = [s / s.std(dim=-1, keepdim=True) for s in mix]  # normalize by the standard deviation\n",
    "    \n",
    "    network.eval()\n",
    "    with torch.no_grad():        \n",
    "        separation = network.inference(mix, n_chunks=2)[-1]  # call the network to obtain the separated audio with shape [1, 4, 1, T']\n",
    "\n",
    "    # normalize the amplitudes by computing the least squares\n",
    "    # -> we try to scale the separated stems so that their sum is equal to the input mix \n",
    "    a = separation[0,:,0,:].cpu().numpy().T  # separated stems\n",
    "    b = mix[-1][0,0,:].cpu().numpy()  # input mix\n",
    "    sol = np.linalg.lstsq(a, b, rcond=None)[0]  # scaling coefficients that minimize the MSE\n",
    "    separation = a * sol  # scale the separated stems\n",
    "\n",
    "    estimates = {\n",
    "        'drums': separation[:,0:1],\n",
    "        'bass': separation[:,1:2],\n",
    "        'other': separation[:,2:3],\n",
    "        'vocals': separation[:,3:4],\n",
    "    }\n",
    "\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for one file\n",
    "\n",
    "audio, rate = librosa.load('Traffic Experiment - Sirens-Copy1.stem.mp4', sr=None)\n",
    "\n",
    "start_pad, stop_pad = max(0, start-4), min(audio.shape[-1]/rate-1, stop+4)\n",
    "start_cut, stop_cut = start-start_pad, stop-stop_pad\n",
    "\n",
    "audio = audio[start_pad*rate:stop_pad*rate].copy()\n",
    "audio = np.expand_dims(audio, 0)\n",
    "\n",
    "song = 'Traffic Experiment - Sirens'\n",
    "print(f\"{song}\")\n",
    "IPython.display.display(IPython.display.Audio(audio[:, start_cut*rate:stop_cut*rate].copy(), rate=rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"separating... \", end='')\n",
    "estimates = separate_sample(audio, rate)\n",
    "estimates = {i: e[start_cut*32000:stop_cut*32000,:] for i, e in estimates.items()}  # cut to show only the desired part (mainly to reduce the latency)\n",
    "print(\"done\")\n",
    "print(\"downloading audio files to the client side...\")\n",
    "\n",
    "for instrument in ['vocals', 'drums', 'bass', 'other']:\n",
    "    if estimates[instrument].max() < 0.25: continue  # hacky way to remove the silent instruments\n",
    "\n",
    "    print(f\"\\n{instrument}\")\n",
    "    IPython.display.display(IPython.display.Audio(estimates[instrument].T.copy(), rate=32000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_test = musdb.DB(root=\"data/Sample\", subsets=\"train\")\n",
    "\n",
    "track_estimates_pairs = []\n",
    "for i, track in enumerate(mus_test.tracks):\n",
    "    estimates = separate_sample(track)\n",
    "    track_estimates_pairs.append((track, estimates))\n",
    "\n",
    "    print(f\"{int((i + 1) / len(mus_test.tracks) * 100)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_estimates_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def evaluate(track_estimates):\n",
    "        track, estimates = track_estimates\n",
    "        museval.eval_mus_track(track, estimates, output_dir=\"/\")\n",
    "\n",
    "pool = multiprocessing.Pool(4)\n",
    "scores_list = list(\n",
    "    pool.imap_unordered(\n",
    "        func=evaluate,\n",
    "        iterable=track_estimates_pairs,\n",
    "        chunksize=1\n",
    "    )\n",
    ")\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(\"Everything is evaluated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
